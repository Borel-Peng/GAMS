<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
    <title>From Coarse-Grained to Super-Class</title>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]},
        "HTML-CSS": { availableFonts: ["TeX"] },
        TeX: { extensions: ["AMSmath.js", "AMSsymbols.js", "boldsymbol.js", "mhchem.js"] }
        });
    </script>

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            background-color: #f5f5f5;
            color: #333;
        }

        img {
            max-width: 100%;
            max-height: 100%;
            height: auto;
            width: auto;
        }

        .roman-numeral {
            font-family: 'Times New Roman', Times, serif;
            font-size: 16px;
        }

        header {
            background-color: white;
            padding: 30px 0;
            color: white;
            text-align: center;
        }

        h1 {
            color: black;
            font-size: 33px;
            line-height: 1.2;
            margin-bottom: 20px;
        }

        p {
            color: #555;
            font-size: 16px;
            margin-bottom: 10px;
        }

        section {
            margin: 40px 0;
            padding: 20px;
            background-color: white;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
        }

        h2 {
            color: #007BFF;
            border-bottom: 2px solid #007BFF;
            padding-bottom: 5px;
            margin-bottom: 20px;
        }

        ul {
            list-style-type: square;
            margin-left: 20px;
        }

        code {
            font-family: 'Courier New', Courier, monospace;
            background-color: #f9f9f9;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 3px;
            display: block;
            overflow-x: auto;
            margin-bottom: 20px;
        }

        figure {
            margin: 20px 0;
        }

        figcaption {
            color: #777;
            font-style: italic;
            text-align: center;
            font-size: 16px;
        }

        footer {
            background-color: white;
            padding: 20px 0;
            color: white;
            text-align: center;
        }
    </style>
</head>

<body>

    <header>
        <h1 style="white-space: pre-line;">
            From Coarse-Grained to Super-Class: 
            Explore the Extremes of Data Granularity
        </h1>
        <div class="author">
            <p>Anonymous Submission</p>
            <!-- <div class="affiliation">
                <p>School of Information, Renmin University of China, Beijing, 100872, China</p>
                <p> School of Statistics, Renmin University of China, Beijing, 100872, China</p>
                <p>Engineering Research Center of Database and Business Intelligence, MOE, China</p>
            </div> -->
        </div>
        <!-- <p>Contact: TODO</p> -->
    </header>

    <div class="container">
        <section id="abstract">
            <h2>1. Abstract</h2>
            <p>Superclass, as the extreme of data granularity, refers to the highest level categorization that encompasses a diverse
                array of basic-level subclasses, which exhibit few shared features in terms of highly abstract semantics. This
                categorization essentially differs from coarse-grained classification methods, which group basic subclasses according to
                identifiable feature commonalities, thereby rendering conventional classification methodologies less or even ineffective
                in superclass problems. This study aims to tackle the challenges in superclass scenarios by leveraging contrastive
                learning. Accordingly, we propose <strong>G</strong>eneralization <strong>A</strong>nalysis <strong>M</strong>echanism of <strong>S</strong>uperclass (<strong>GAMS</strong>) to uncover the fundamental
                principles of superclass generalization. Specifically, GAMS theoretically unveils the hierarchical intricacy of
                superclass learning and proves that the superclass generalization is controlled by three measures: sample diversity,
                subclass alignment and superclass divergence. Moreover, GAMS explicitly reveals that compared to conventional
                coarse-grained classification methods, simultaneously aligning subclasses and diverging superclasses contribute to lower
                model complexity and better generalization. Guided by these findings, we propose a novel method for superclass learning,
                and extensive experiments on 9 datasets confirm the advantages of our proposed method.</p>
        </section>

        <section id="problem">
            <h2>2. Problem Definition</h2>
            <p>
            Recent advances in visual categorization have primarily addressed basic-level classification (e.g., distinguishing cats
            from dogs), while super-coarse-grained categorization (superclass learning) remains understudied. Superclass
            classification involves grouping diverse subclasses under abstract semantic categories (e.g., recyclable vs. kitchen
            waste), where subclasses share few discernible features. Traditional methods fail in this paradigm due to two
            fundamental challenges: 1) Extreme data diversity from heterogeneous subclasses within each superclass, and 2) Violation
            of the smoothness assumption as samples from different superclasses may share indistinguishable features while requiring
            distinct classifications (e.g., real apples vs. toy apples in waste sorting in Figure 1).
            </p>

            <div style="display: flex; justify-content: center; flex-wrap: wrap;">
                <div class="ablation-item" style="flex: 1; max-width: 40%; margin: 0 20px;">
                    <img class="img_responsive" src="./waste-0317.svg" alt="refuse" style="width: 100%;">
                    <figcaption style="text-align: center; margin-top: 8px;">Figure 1: Example of superclass: refuse sorting</figcaption>
                </div>
            </div>
                        <p>
                            We establish a Generalization Analysis Mechanism for Superclass (GAMS), the first theoretical framework for
                            superclass
                            learning. GAMS identifies three key factors: (1) Data diversity, decomposed into subclass variety and intra-subclass
                            heterogeneity, which quantifies classification complexity; (2) Subclass alignment and superclass divergence as dual
                            mechanisms for generalization enhancement; (3) Version space compactness, demonstrating that unified
                            alignment-divergence optimization enables tighter hypothesis spaces than traditional two-stage approaches.
                            Theoretically, GAMS proves that model generalization is bounded by these factors, providing principled guidance for
                            algorithm design.
                        </p>
                        <p>
                            Guided by GAMS, we propose Superclass Learning with Representation Enhancement (SCLRE), which jointly optimizes
                            subclass
                            alignment and superclass divergence through novel loss functions. Our framework theoretically reduces version space
                            complexity while improving generalization bounds, addressing the core challenges of superclass learning through a
                            unified representation enhancement paradigm.
                        </p>
        </section>

<section id="main-results">
    <h2>3. <strong>GAMS</strong><strong>: G</strong>eneralization <strong>A</strong>nalysis <strong>M</strong>echanism
        of <strong>S</strong>uperclass</h2>

    <p>Superclass data can be categorized into three levels: samples, subclasses, and superclasses. The question is, how
        does hierarchical superclass data differ from ordinary coarse-grained classes? In this section, we offer a
        detailed Generalization Analysis Mechanism for Superclass (GAMS).</p>
    <ul>
        <li>
            <span style="color: green;"><u>(<span class="roman-numeral">Subsection III-B</span>)</u></span> <span
                style="color: red;"> <strong>Difficulty for Superclass Classification.</strong></span>
            <p>Why do traditional coarse-grained classification methods perform poorly on superclass data? One main
                reason lies in the
                diversity of superclasses. Diversity can be understood from two aspects: (1) the number of subclasses
                within the
                superclass, and (2) the diversity within each subclass.
            </p>
            <p> The more subclasses there are in the superclass, and the richer each subclass is, the more complex the
                classification
                boundaries for superclasses should be. See figure 3.
            </p>
            <div class="img" style="text-align:center">
                <img class="img_responsive" src="./def1-v2.svg" alt="def1" style="margin:auto;max-width:40%">
                <figcaption>Figure 3: Illustration of $\delta$-diversity .
                </figcaption>
            </div>
        </li>
        <span style="color: red;"><u><strong>Definition: $v(\delta)$-diversity</strong></u></span><span style="font-style: italic;"> A superclass dataset is called $v(\delta)$-diverse if there exists a positive
            $\delta$ such that among all the
            subclasses in the superclass, there is a subset $S_\delta\subset \underset{i,j}{\cup} S_{i,j}$, in which the
            shortest
            relative distance between every pair of subclasses is less than $\delta$ and the ratio of $S_\delta$ is
            $v(\delta)$.
            Specifically,
        </span>
        $$
        v(\delta)=\mathbb{P}\left[\boldsymbol{x}\in S_\delta, \forall S_{i,j}, S_{k,l}\subset S_\delta,
        \frac{\text{dist}( S_{i,j},S_{k,l})}{\Vert\boldsymbol{\mu}_{i,j}-\boldsymbol{\mu}_{k,l}\Vert}\le\delta \right],
        $$
        <p style="font-style: italic;">
            where $\boldsymbol{\mu}_{i,j}, \boldsymbol{\mu}_{k,l}$ are subclass centers and
            $\text{dist}(S_{i,j},S_{k,l})$ is
            the distance between the closest elements within $S_{i,j}$ and $S_{k,l}$, that is,
            $\text{dist}(S_{i,j},S_{k,l})=\underset{\boldsymbol{x}\in S_{i,j},\boldsymbol{y}\in S_{k,l}}{\inf}\Vert
            \boldsymbol{x}-\boldsymbol{y}\Vert$.
        </p>

        <p style="color: blue;">A smaller $\delta$ and a larger $v(\delta)$ correspond respectively to a higher
            diversity within an individual subclass
            and a higher diversity across the entire superclass. In other words, we can add more varied samples within a
            subclass to
            reduce $\delta$, and incorporate more diverse subclasses into the superclass to increase $v(\delta)$.
            When the data becomes more diverse, the classification boundary is blurred and the classification task
            becomes
            increasingly challenging, as is illustrated next:
        </p>

        <span style="color: red;"><u><strong>Theorem 1</strong></u></span><span style="font-style: italic;"> Given a hypothesis set $\mathcal{H}$ containing $L$-Lipschitz continuous encoder
            $f$, for any superclasses dataset
            $\mathcal{D}$ with $v(\delta)$ diversity, if there exists a $f\in \mathcal{H}$ can distinguish all diverse
            subsets
            $S_\delta$ within $\mathcal{D}$ (i.e. images of different superclasses under $f$ should be orthogonal), then
            the
            following two bounds hold for $\mathcal{D}$:
        </span>
        $$
        \delta\ge\frac{\sqrt{2}}{C\cdot L}, v(\delta)\le\sqrt{\frac{L\cdot B\cdot d}{\sqrt{2}K^3m^3}+\frac{1}{K}},
        $$
        <p style="font-style: italic;">
            where $B$ is a bound on the data distribution, $B\le1$, and $d$ is the embedding dimension.
        </p>

        <span style="color: blue;"> It states that, for a given model, if we want it to fully recognize a superclass
            dataset, there must be an upper bound on
            the diversity of the superclass dataset. In other words, for a given model, if the diversity of the
            subclasses $ \delta
            $ or the diversity of the superclasses $ v(\delta) $ exceeds a certain threshold, the model will inevitably
            fail to
            classify the whole dataset.
            Moreover, as $ K $ and $ m $ increase, the threshold of $ v(\delta) $ tightens.
            This indicates that an increase in the number of superclasses and subclasses improves the difficulty of
            classification
            task for a certain model.</span>

        <li> <span style="color: green;"><u>(<span class="roman-numeral">Subsection III-C</span>)</u></span> <span
                style="color: red;"> <strong>Metrics for Alignment and Divergence.</strong></span>
        </li>
        <p>For standard classification tasks, we aim for the model to align all samples with the same label. However,
            for
            superclass data, because of its extreme diversity, it is exceedingly challenging to align the entire
            superclass.
            To address this, we relax the requirements for the classifier: instead of aligning the entire superclass, we
            focus on
            (1) aligning samples within most subclasses and (2) ensuring that the centers of subclasses from different
            superclasses
            remain as distinct as possible.
            Based on these two objectives, we propose the following two metrics:</p>

        <span style="color: red;"><u><strong>Subclass alignment</strong></u></span><span> A model with good <em>alignment</em> ability should produce similar outputs for
            similar samples. Mathematically, if a
            model can map an entire subclass into an $ \epsilon $-ball, we say that the model can $ \epsilon $-align
            this
            subclass. Further, we denote $ S_\epsilon $ as the union of all subclasses that can be $\epsilon$-aligned.
        </span>
        <p></p>
        <span style="color: red;"><u><strong>Definition: $\epsilon$-align</strong></u></span><span style="font-style: italic;"> Given a superclass classifier $f$ mapping the original samples into a new
            representation space. We call $f$ can
            $\epsilon$-align a subclass $S_{i,j}$, if</span>
        $$
        \text{diam}(f(S_{i,j}))\le \epsilon,
        $$
        <p style="font-style: italic;">where $\text{diam}(\cdot)$ measures the diameter of a set.</p>

        <p style="font-style: italic;">Specifically, $S_\epsilon$ denotes all the subclasses which can be
            $\epsilon$-aligned by the encoder:</p>
        $$
        S_\epsilon=\underset{i \in[K]}{\cup}\underset{j\in[m_i]}{\cup}S_{i, j}^*,\ s.t.,\text{diam}(f(S_{i, j}^*))\le
        \epsilon.
        $$

        <p style="font-style: italic;">Furthermore, given a $\delta$-diverse dataset, $R_\epsilon$ denotes the ratio of
            <em>diverse</em> subclasses which
            $f$ can not align:</p>
        $$
        R_\epsilon=1-\mathbb{P}\left[S_\epsilon\cap S_\delta\right].
        $$

        <span style="color: red;"><u><strong>Superclass Divergence</strong></u></span><span> Simply aligning subclasses is not sufficient for effective superclass learning.
            We define the concept of superclass
            divergence:</span>
        <p></p>
        <span style="color: red;"><u><strong>Definition: Superclass Divergence</strong></u></span><span style="font-style: italic;"> Denote $\boldsymbol{\mu}_k$ as each superclass center in the representation
            space. The divergence of an encoder can
            be quantified by:</span>
        $$
        \sigma=\underset{i,j\in[K]}{\max}\boldsymbol{\mu}_i^T\boldsymbol{\mu}_j.
        $$
        <p style="color: blue;">We illustrate the definitions below.</p>
        <div class="img" style="text-align:center">
            <img class="img_responsive" src="./def23-0309.svg" alt="def23" style="margin:auto;max-width:40%">
            <figcaption>Figure 4: Illustration of Definition 2 and 3.
            </figcaption>
        </div>
        <span style="color: red;"><u><strong>Theorem 2</strong></u></span><span style="font-style: italic;"> Given a classifier $G_f$ with an encoder $f$, if the data diversity $v(\delta)$,
            subclass alignment $R_\epsilon$,
            superclass divergence $\sigma$ satisfy:</span>
        $$
        aR_\epsilon^2+b\sigma^2-cv^2(\delta)\le C.
        $$

        <p style="font-style: italic;">Then, the expectation of downstream classification error is upper-bounded:</p>
        $$
        \text{Err}(G)=\sum_{k=1}^K\mathbb{P}\left[G_f(\boldsymbol{x})\neq k, \boldsymbol{x}\in S_k\right]\le 1-
        v(\delta)+R_\epsilon.
        $$

        <p style="font-style: italic;">Where $a,b,c$ are positive constants decided by $\epsilon$ and $\delta$ and $C$
            contains constants and linear/cross
            terms. The exact form refers to the appendix.</p>


        <p style="color: blue;">Theorem2 implies that improving the model’s alignment and increasing data diversity enhances generalization. If \cref{thm:
        restrict for bound} holds, the model’s generalization error on superclass data will decrease linearly as \( v(\delta) \)
        increases and \( R_\epsilon \) decreases. Meanwhile, superclass divergence is necessary for strong generalization. The
        validity of \cref{thm: restrict for bound} requires three factors: a small \( \sigma \), a small \( R_\epsilon \), and a
        large \( v(\delta) \). Even if the model exhibits strong alignment and the dataset is sufficiently diverse, poor
        superclass divergence (i.e., a large \( \sigma \)) can lead to the failure of generalization.</p>
            <div class="img" style="text-align:center">
                <img class="img_responsive" src="./thm2-0309.svg" alt="thm2" style="margin:auto;max-width:40%">
                <figcaption>Figure 5: Illustration of Thm 2.
                </figcaption>
            </div>
        <span style="color: red;"><u><strong>Theorem 3</strong></u></span> <span style="font-style: italic;">For the same superclass dataset $\mathcal{D}$, let $\mathcal{H}_{1}$ be the minimal hypothesis set aligning and
        diverging $\mathcal{D}$, and $\mathcal{H}_{2}$ be the minimal hypothesis set distinguishing all subclasses, then,</span>
        $$
        \mathcal{H}_1\subset\mathcal{H}_2,\ \Pi_{\mathcal{H}_1}(N)<<\ \Pi_{\mathcal{H}_2}(N),
        $$
        <span style="color: red;"><u><strong>Corollary 3.1</strong></u></span><span style="font-style: italic;"> The generalization error of $\mathcal{H}_1$ on any superclass datasets is smaller
            than that of $\mathcal{H}_2$.</span>
                    <div style="display: flex; justify-content: center; flex-wrap: wrap;">
                        <div class="ablation-item" style="flex: 1; max-width: 40%; margin: 0 20px;">
                            <img class="img_responsive" src="./thm3-0309.svg" alt="thm3" style="width: 100%;">
                            <figcaption style="text-align: center; margin-top: 8px;"></figcaption>
                        </div>
                    </div>
        <span style="color: blue;">
            <p style="color: blue;">\cref{fig:thm3} illustrates the relationship between \( \mathcal{H}_1 \) and \( \mathcal{H}_2 \). As model complexity
            increases, the training loss decreases, and the gap between training loss and test loss narrows. \( \mathcal{H}_0 \)
            represents the model with insufficient complexity, which, according to \cref{thm: complexity between normal class and
            superclass:2}, has relatively high training loss. In contrast, both \( \mathcal{H}_1 \) and \( \mathcal{H}_2 \) achieve
            near-zero training loss, but the unnecessarily high complexity of \( \mathcal{H}_2 \) leads to an increase in
            generalization loss.
            \cref{fig:nested} offers another perspective to \cref{thm: complexity between normal class and superclass:2}.
            \(\mathcal{H}_0\subset\mathcal{H}_1\subset\mathcal{H}_2\), however, \(\mathcal{H}_0\) as a hypothesis space does not
            contain the optimal solution, resulting in a relatively large generalization error for any model within it. On the other
            hand, both \(\mathcal{H}_1\) and \(\mathcal{H}_2\) contain the global optimal solution. However, since \(\mathcal{H}_1\)
            is smaller, it is easier to find this global optimal solution. Therefore, \(\mathcal{H}_2\) is superior to
            \(\mathcal{H}_3\).</p>
        </span>
                                <div class="img" style="text-align:center">
                                    <img class="img_responsive" src="./hypo-0306.svg" alt="hypo" style="margin:auto;max-width:40%">
                                    <figcaption>
                                    </figcaption>
                                </div>
        </li>

    </ul>

</section>

        <section id="application">
            <h2>4. SuperClass Learning with Representation Enhancement (SCLRE)</h2>
            <p>Based on our theoretical framework, we propose a novel method called <span style="color: green;"><u>SCLRE
                        (Section <span class="roman-numeral">IV</span>)</u></span> that achieves both subclass alignment
                and superclass divergence without explicitly identifying all subclasses.</p>

            <ul>
                <li><span style="color: green;"><u>(<span class="roman-numeral">Subsection IV-A</span>)</u></span> <span
                        style="color: red;"><strong>Feature Reconstruction.</strong></span>
                    <br>
                    SCLRE uses a cross-instance attention module to reconstruct the feature space, capturing
                    relationships between representations of different instances rather than features within instances:
                    $$
                    \begin{aligned}
                    a_{ij} = \frac{\exp(q_i k_j^T/\sqrt{D})}{\sum_{k=1}^{N} \exp(q_i k_k^T/\sqrt{D})}
                    \end{aligned}
                    $$

                    <span style="color: blue;">This mechanism generates enhanced representations that better reflect the
                        superclass structure</span>.
                    <br>
                    <br>
                </li>

                <li><span style="color: green;"><u>(<span class="roman-numeral">Subsection IV-B</span>)</u></span> <span
                        style="color: red;"><strong>Representation Enhancement.</strong></span>
                    <br>
                    SCLRE employs two specialized loss functions:
                    <br>

                    1. <strong>Subclass Alignment Loss ($\mathcal{L}_{\text{alg}}$)</strong>:
                    $$
                    \begin{aligned}
                    \mathcal{L}_{\text{alg}} = -\frac{1}{N}\sum_{i=1}^{N} \log \frac{\exp(s(z_i, t(z_i))/\tau)}{\sum_{t\in T}\exp(s(z_i,
                    t)/\tau)}
                    \end{aligned}
                    $$
                    <br>
                    2. <strong>Superclass Divergence Loss ($\mathcal{L}_{\text{div}}$)</strong>:
                    $$
                    \begin{aligned}
                    \mathcal{L}_{\text{div}} = \frac{1}{|S(z_i)| - 1}\sum_{i=1}^{N}\sum_{z^+\in S(z_i)\backslash\{z_i\}} \ell(z_i, z^+)
                    \end{aligned}
                    $$
                    <br>
                    <span style="color: blue;">These losses ensure that samples within the same superclass are aligned
                        and samples from different superclasses are well-separated</span>.
                    <br>
                    <br>
                </li>

                <li><span style="color: green;"><u>(<span class="roman-numeral">Subsection IV-C</span>)</u></span> <span
                        style="color: red;"><strong>Representation Decoding.</strong></span>
                    <br>
                    The final loss function of SCLRE combines cross-entropy loss with the specialized alignment and
                    divergence losses:
                    $$
                    \begin{aligned}
                    \mathcal{L}_{\text{SCLRE}} = (1 - \alpha)\mathcal{L}_{\text{ce}} + \alpha \mathcal{L}_{\text{div}} + \lambda \mathcal{L}_{\text{alg}}, \alpha, \lambda \in (0, 1)
                    \end{aligned}
                    $$

                    <span style="color: blue;">This combined loss enables SCLRE to adjust representations properly,
                        running smoothly and converging steadily</span>.
                    <br>
                        <div class="img" style="text-align:center">
                            <img class="img_responsive" src="./model.svg" alt="SCLRE model"
                                style="margin:auto;max-width:40%">
                            <figcaption>Figure 5: Illustration of the SCLRE algorithm.
                            </figcaption>
                        </div>
                    <br>
                    
                </li>

                <li><span style="color: green;"><u>(<span class="roman-numeral">Subsection IV-D</span>)</u></span> <span
                        style="color: red;"><strong>Theoretical Analysis.</strong></span>
                    <br>
                    We prove that minimizing $\mathcal{L}_{\text{SCLRE}}$ enhances both alignment and divergence:
                    <br>
                    <span style="color: red;"><u><strong>Theorem 4</strong></u></span> <span style="font-style: italic;"">For any $\epsilon>0$,</span>
                    $$
                        R_\varepsilon^2 \leq \eta(\varepsilon) \cdot \mathcal{L}_{\text{alg}},
                    $$
                    <span style="color:red;"><strong><u>Theorem 5</u></strong></span> <span style="font-style: italic;">For fixed $\delta,\epsilon$, the following bound holds:</span>
                    $$
                        \sigma \leq \log \left( \exp \left\{ C({ \mathcal{L}_{\text{div}} + \tau(R_\varepsilon, \delta, \varepsilon)})
                        \right\} - \exp(1 - \varepsilon) \right),
                    $$
                    <span style="color: blue;">These theorems demonstrate that $\mathcal{L}_{SCLRE}$ enhances both the alignment and divergence of the model. When superclass data is
                    highly diverse and $\mathcal{L}_{\text{div}}$ and $\mathcal{L}_{\text{div}}$ are minimized, the conditions from the
                    Theorem2 can be satisfied, thereby keeping the model’s generalization error under control.</span>.
                </li>
                
            </ul>
        </section>

        <section id="experiment-results">
            <h2>5. Experiments</h2>
            <ul>
                <li>
                    <span style="color: green;"><u>(<span class="roman-numeral">Subsection V-A</span>)</u></span> <span
                        style="color: red;"> <strong>Datasets.</strong></span> We evaluate our method on artificially
                    reorganized superclass datasets from three benchmarks (CIFAR-100, mini-ImageNet, and VOC) and three
                    real-world datasets (FMoW, Adience, and Image Aesthetic dataset).

                    <div class="img" style="text-align:center">
                        <img class="img_responsive" src="./dataset.svg" alt="CIFAR100-4 details"
                            style="margin:auto;max-width:40%">
                        <figcaption>Figure 6: Details of CIFAR100-4, showing the mapping between original classes and
                            superclasses.</figcaption>
                    </div>
                </li>

                <li>
                    <span style="color: green;"><u>(<span class="roman-numeral">Subsection V-B</span>)</u></span> <span
                        style="color: red;"> <strong>Results on Artificially Reorganized Datasets.</strong></span>
                    <div class="img" style="text-align:center">
                        <img class="img_responsive" src="./artificial_dataset_results.png" alt="Artificial dataset results"
                            style="margin:auto;max-width:80%">
                        <figcaption>Table 1: Accuracy on artificially reorganized datasets. SCLRE significantly outperforms
                            baseline methods on most datasets.</figcaption>
                    </div>
                </li>

                <li>
                    <span style="color: green;"><u>(<span class="roman-numeral">Subsection V-B</span>)</u></span> <span
                        style="color: red;"> <strong>Results on Real-World Datasets.</strong></span>
                    <div class="img" style="text-align:center">
                    <img class="img_responsive" src="./real_world_results.png" alt="Real-world results"
                        style="margin:auto;max-width:80%">
                    <figcaption>Table 2: Accuracy on real-world datasets. SCLRE outperforms both traditional methods and
                        multimodal pre-trained models like CLIP.</figcaption>
                    </div>    
                </li>

                <li>
                    <span style="color: green;"><u>(<span class="roman-numeral">Subsection V-B</span>)</u></span> <span
                        style="color: red;"> <strong>Visualization Results.</strong></span>
                    <div class="img" style="text-align:center">
                        <img class="img_responsive" src="./cifar10.svg" alt="Visualization"
                            style="margin:auto;max-width:80%">
                        <figcaption>Figure 7: Visualization results using t-SNE on CIFAR10-2. SCLRE effectively clusters
                            images from the same superclass while separating different superclasses.</figcaption>
                    </div>
                </li>

                <li>
                    <span style="color: green;"><u>(<span class="roman-numeral">Subsection V-C & V-D</span>)</u></span>
                    <span style="color: red;"> <strong>Ablation Studies and Sensitivity Analysis.</strong></span>
                    
                </li>
                <div class="ablation-studies" style="margin-top: 20px;">
                    <div style="display: flex; justify-content: space-between; margin-bottom: 20px; flex-wrap: wrap;">
                        <div class="ablation-item" style="flex: 1; min-width: 30%; margin: 0 5px;">
                            <img class="img_responsive" src="./alpha.png" alt="Alpha parameter ablation" style="width: 100%;">
                            <figcaption style="text-align: center; margin-top: 8px;">(a) Effect of alpha parameter</figcaption>
                        </div>
                        <div class="ablation-item" style="flex: 1; min-width: 30%; margin: 0 5px;">
                            <img class="img_responsive" src="./lambda.png" alt="Lambda parameter ablation" style="width: 100%;">
                            <figcaption style="text-align: center; margin-top: 8px;">(b) Effect of lambda parameter</figcaption>
                        </div>
                        <div class="ablation-item" style="flex: 1; min-width: 30%; margin: 0 5px;">
                            <img class="img_responsive" src="./batch.png" alt="Batch size ablation" style="width: 100%;">
                            <figcaption style="text-align: center; margin-top: 8px;">(c) Effect of batch size</figcaption>
                        </div>
                    </div>
                
                    <div style="display: flex; justify-content: center; flex-wrap: wrap;">
                        <div class="ablation-item" style="flex: 1; max-width: 45%; margin: 0 5px;">
                            <img class="img_responsive" src="./loss.png" alt="Loss function ablation" style="width: 100%;">
                            <figcaption style="text-align: center; margin-top: 8px;">(d) Effect of loss function</figcaption>
                        </div>
                        <div class="ablation-item" style="flex: 1; max-width: 45%; margin: 0 5px;">
                            <img class="img_responsive" src="./backbone.png" alt="Backbone ablation" style="width: 100%;">
                            <figcaption style="text-align: center; margin-top: 8px;">(e) Effect of backbone architecture</figcaption>
                        </div>
                    </div>
                </div>
            </ul>
        </section>

        <section id="contribution">
            <h2>6. Contributions</h2>
            <ul>
                TODO
            </ul>
        </section>

        <section id="code">
            <h2>7. Code</h2>
            <p>Access our research codebase at <a href="" target="_blank">TODO.</a>
            </p>
        </section>
    </div>

    <footer>
        <p>&copy; 2025 TODO, Renmin University of China, Beijing, China</p>
    </footer>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

</body>

</html>